{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09b174ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutup;\n",
    "shutup.please()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f15741d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "\n",
    "# nlp libraries\n",
    "import spacy\n",
    "import de_core_news_sm\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from textblob_de import TextBlobDE as TextBlob\n",
    "import string\n",
    "import re\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim import corpora\n",
    "import pyLDAvis\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "# visualization libraries\n",
    "import plotly.subplots as sp\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "# interactive widgets\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Layout\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5ed9cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/michaelstark/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c94097f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jupyter matplotlib backend magic\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d1efee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for reading in the dataset according to year input\n",
    "\n",
    "def read_textfile(year):\n",
    "    file = pd.read_csv(f'fem_df{year}.csv')\n",
    "    return file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90ae170f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions performing nlp - analysis\n",
    "\n",
    "# loading german corpus from spacy library\n",
    "global nlp\n",
    "\n",
    "nlp = de_core_news_sm.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "328f2542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for cleaning and splitting input texts for further processing\n",
    "\n",
    "def file_io(filename):\n",
    "    f_io = filename.split('.')\n",
    "    f_c = [re.sub('\\\\n', '', f) for f in f_io]\n",
    "    return f_c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5317cb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that performs pos-tagging on sentence and returns list with pos-tags\n",
    "\n",
    "def pos_tagger(sent):\n",
    "    doc = nlp(sent)\n",
    "    pos_tags = [token.tag_ for token in doc]\n",
    "    return pos_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6405324b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that searches string for named entities and returns list of entities\n",
    "\n",
    "def recog_ne(sent):\n",
    "    doc = nlp(sent)\n",
    "    named_ents = [ent.label_ for ent in doc.ents]\n",
    "    return named_ents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "760e99ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that performs sentiment analysis on a given string and returns polarity\n",
    "\n",
    "def sent_analysis(sent):\n",
    "    blob = TextBlob(sent)\n",
    "    return blob.sentiment[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "746fbe66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that creates a wordcloud derived from topic modeling\n",
    "\n",
    "def wc_from_tm(input_string, num_topics=1):\n",
    "    # Process the input string using the German language model\n",
    "    doc = nlp(input_string)\n",
    "\n",
    "    # Tokenize the input string, removing stop words and punctuation\n",
    "    tokens = [token.lemma_ for token in doc\n",
    "              if not token.is_stop and not token.is_punct]\n",
    "\n",
    "    # Create a dictionary from the tokenized input\n",
    "    dictionary = corpora.Dictionary([tokens])\n",
    "\n",
    "    # Create a bag of words representation of the input\n",
    "    bow = [dictionary.doc2bow(tokens)]\n",
    "\n",
    "    # Train the LDA model on the input\n",
    "    lda_model = LdaModel(bow, num_topics=num_topics, id2word=dictionary)\n",
    "\n",
    "    topic_distribution = lda_model[bow]\n",
    "    # Create a word cloud for each topic\n",
    "    for topic_id, topic_prob in topic_distribution[0]:\n",
    "        topic = lda_model.show_topic(topic_id)\n",
    "        topic_words = \" \".join([word for word, prob in topic])\n",
    "        wordcloud = WordCloud(background_color='white',\n",
    "                              width=400,\n",
    "                              height=200,\n",
    "                              max_words=200,\n",
    "                              max_font_size=50,\n",
    "                              min_font_size=12).generate(topic_words)\n",
    "        \n",
    "    return wordcloud.to_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e0e1eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function for creating an ordered, counted dictionary\n",
    "\n",
    "def count_dict(obj_to_count_from):\n",
    "    c = Counter(obj_to_count_from)\n",
    "    sort_c = sorted(c.items(), key=lambda x:x[1], reverse=True)\n",
    "    sort_dict_c = {obj[0]: obj[1] for obj in sort_c}\n",
    "    return sort_dict_c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2aea3662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for visualizing the data\n",
    "\n",
    "def visualize_data(text_num=0, year=2019):\n",
    "    \n",
    "    if text_num > len(read_textfile(year)):\n",
    "        return\n",
    "    \n",
    "    inp_string = read_textfile(year)\n",
    "    \n",
    "    text_to_vis = inp_string[f'articles_{year}'][text_num]\n",
    "    \n",
    "    # prepare necessary data\n",
    "    text_to_analyze = file_io(text_to_vis)\n",
    "\n",
    "    pos_tag_count = count_dict(pos_tagger(text_to_vis))\n",
    "\n",
    "    # count the named entities\n",
    "\n",
    "    ne = []\n",
    "\n",
    "    for sen in text_to_analyze:\n",
    "        ne += recog_ne(sen)\n",
    "\n",
    "    counted_na_ent = count_dict(ne)\n",
    "\n",
    "\n",
    "    # sent_analysis for whole article\n",
    "\n",
    "    sent_an = [sent_analysis(sent) for sent in text_to_analyze]\n",
    "\n",
    "    # make dataframes for visualizations\n",
    "\n",
    "    make_df = {'Pos Tags':pos_tag_count.keys(),\n",
    "               'Count': pos_tag_count.values()}\n",
    "\n",
    "    make_df_na = {'Named Entities':counted_na_ent.keys(),\n",
    "                  'Count': counted_na_ent.values()}\n",
    "\n",
    "    df_make_sa = {'Sentence No.': list(range(len(sent_an))),\n",
    "                  'Sentiment Polarity': sent_an}\n",
    "\n",
    "    df_sa = pd.DataFrame(df_make_sa)\n",
    "    df_na = pd.DataFrame(make_df_na)\n",
    "    df_pt = pd.DataFrame(make_df)\n",
    "    wc_array = wc_from_tm(text_to_vis)\n",
    "\n",
    "\n",
    "    # making the subplots\n",
    "    fig_sb = sp.make_subplots(rows=2,\n",
    "                              cols=2,\n",
    "                              subplot_titles=(\"Pos Tags\",\n",
    "                                              \"Named Entites\",\n",
    "                                              \"Sentiment Analysis\",\n",
    "                                              \"WordCloud from Topic Modeling\"),\n",
    "                              specs=[[{'type':'bar'}, {'type':'domain'}],\n",
    "                                    [{'type':'xy'}, {}]])\n",
    "\n",
    "\n",
    "        # first one = barplot pos_tags\n",
    "    fig_pt = go.Bar(x = df_pt['Pos Tags'],\n",
    "             y = df_pt['Count'],\n",
    "             marker_color=list(range(len(df_pt['Pos Tags'])))[::-1])\n",
    "\n",
    "\n",
    "        # second one = pieplot named entities\n",
    "    fig_na = go.Pie(labels=df_na['Named Entities'],\n",
    "                    values=df_na['Count'])\n",
    "\n",
    "\n",
    "    # third one = line plot sentiment analysis\n",
    "\n",
    "    fig_sa = go.Scatter(x=df_sa['Sentence No.'],\n",
    "                        y = df_sa['Sentiment Polarity'])\n",
    "\n",
    "\n",
    "    fig_wc = go.Image(z=wc_array, hoverinfo='none')\n",
    "\n",
    "\n",
    "        # adding to main plot\n",
    "\n",
    "    fig_sb.add_trace(fig_pt, row=1, col=1)\n",
    "    fig_sb.add_trace(fig_na, row=1, col=2)\n",
    "    fig_sb.add_trace(fig_sa, row=2, col=1)\n",
    "    fig_sb.add_trace(fig_wc, row=2, col=2)\n",
    "\n",
    "    # styling plot\n",
    "    fig_sb.update_xaxes(tickfont_size=10,  row=1, col=1)\n",
    "    fig_sb.update_xaxes(visible=False,  row=2, col=1)\n",
    "    fig_sb.update_xaxes(visible=False,  row=2, col=1)\n",
    "    fig_sb.update_xaxes(visible=False,  row=2, col=2)\n",
    "\n",
    "    fig_sb.update_yaxes(range=[-1, 1], row=2, col=1)\n",
    "    fig_sb.update_yaxes(visible=False,  row=2, col=2)\n",
    "    \n",
    "    fig_sb.update_annotations(yshift=12, \n",
    "                              font=dict(family='Courier New, monospace', \n",
    "                                        size=14, color='Black'))\n",
    "\n",
    "\n",
    "    fig_sb.update_layout(\n",
    "        width = 1350,\n",
    "        height = 600,\n",
    "        title='NLP Dashboard for Media Coverage of Femicides in Austria',\n",
    "        title_font=dict(\n",
    "            family='Courier New, monospace',\n",
    "            size=18,\n",
    "            color='Black'\n",
    "            ),\n",
    "        showlegend=False\n",
    "        )\n",
    "\n",
    "    return fig_sb.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82342217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating widgets\n",
    "\n",
    "\n",
    "text_select = widgets.BoundedIntText(\n",
    "                                    value=0,\n",
    "                                    min=0,\n",
    "                                    max=50,\n",
    "                                    step=1,\n",
    "                                    description='Article No.',\n",
    "                                    disabled=False)\n",
    "\n",
    "\n",
    "year_select = widgets.BoundedIntText(\n",
    "                                     value=2019,\n",
    "                                     min=2019,\n",
    "                                     max=2022,\n",
    "                                     step=1,\n",
    "                                     description='Year:',\n",
    "                                     disabled=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74f9143",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align:center; font-family:Courier New, monospace; font-weight:bold;\">F.E.M.</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd84ceeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec14bd073d8f4d99b450735291ff1328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(BoundedIntText(value=0, description='Article No.', max=50), BoundedIntText(value=2019, d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "int_act = widgets.interact(visualize_data, text_num=text_select, year=year_select)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
